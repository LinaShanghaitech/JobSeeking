### 机器学习  

- [ ]  K-Means、K-Means++、KNN
- [ ]  Word2Vec
- [ ]  PCA、LDA、常用降维算法 
- [ ]  EM算法 
- [ ]  Logistic回归、Softmax回归
- [ ]  SVM 优化目标、软间隔、调参、SVM核函数原理、如何解决多分类问题
- [ ]  鞍点问题  
- [ ]  L0、L1、L2正则  

[完整知识点](https://mp.weixin.qq.com/s?__biz=MzUxNjcxMjQxNg==&mid=2247488644&idx=2&sn=e6a9d62e7abe5cff4a7c9d84a6d1bdf5&chksm=f9a2660bced5ef1d8e347b63a939aa893349e76796311d4a729340fbc6a3d4ba0cc64978ba9f&scene=21#wechat_redirect)


- [ ] 常用优化算法  
- 梯度下降法      
- 牛顿法   
- 拟牛顿法     
- 梯度下降法的改进: Adam\AdaGrad\  
- [ ]  拉格朗日乘数法  
- [ ]  凸优化算法: 线性回归、岭回归、LASSO回归、Logistic回归、支持向量机、Softamx回归  
- [ ]  拉格朗日对偶  
- [ ]  KKT条件
- [ ]  特征值与特征向量、奇异值分解
- [ ]  最大似然估计  
- [ ]  贝叶斯分类器  
- [ ]  有监督学习、无监督学习  
- [ ]  分类问题、回归问题  
- [ ]  生成模型、判别模型  
- [ ]  交叉验证  
- [ ]  过拟合、欠拟合
- [ ]  偏差与方差分解  
- [ ]  维度灾难  
- [ ]  决策树   
- [ ]  高斯混合模型  
- [ ]  循环神经网络  
- [ ]  主成分分析、线性判别分析  
- [ ]  复合函数求导和梯度下降  
